{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "08c0cd81",
   "metadata": {},
   "source": [
    "# AI-Powered Micro-Activity Recommendation System  \n",
    "### Using Reinforcement Learning and Hybrid AI\n",
    "\n",
    "**Student Name:**  \n",
    "**Project Track:** Recommendation Systems / Applied ML  \n",
    "**Mentor:**  \n",
    "\n",
    "This notebook presents the design, implementation, and evaluation of a personalized micro-activity recommendation system that learns from user feedback using reinforcement learning."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de10190d",
   "metadata": {},
   "source": [
    "## 1. Problem Definition & Objective\n",
    "\n",
    "### a. Selected Project Track\n",
    "This project falls under the **Recommendation Systems / Applied Machine Learning** track.\n",
    "\n",
    "### b. Problem Statement\n",
    "Users often waste short free time slots due to decision fatigue and lack of personalized suggestions. Existing productivity tools provide static or generic recommendations that do not adapt to individual preferences.\n",
    "\n",
    "This project aims to build an AI system that recommends short, context-aware activities and continuously improves its suggestions using user feedback.\n",
    "\n",
    "### c. Real-World Relevance & Motivation\n",
    "Micro-moments (5–30 minutes) are common in daily life, especially for students and professionals. Efficient use of these moments can improve productivity, mental well-being, and habit formation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6557481",
   "metadata": {},
   "source": [
    "## 2. Data Understanding & Preparation\n",
    "\n",
    "### a. Dataset Source\n",
    "The dataset used in this project is a **custom curated synthetic dataset** of micro-activities.\n",
    "It was manually designed to simulate real-world recommendation scenarios.\n",
    "\n",
    "Each activity contains:\n",
    "- Activity name\n",
    "- Category\n",
    "- Suitable context (energy, location, duration)\n",
    "- Description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7ee7f3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "with open(\"activities_dataset.json\", \"r\") as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c269b2bd",
   "metadata": {},
   "source": [
    "### b. Data Exploration\n",
    "The dataset consists of ~50–60 activities across multiple categories such as:\n",
    "- Physical\n",
    "- Mental\n",
    "- Creative\n",
    "- Relaxation\n",
    "- Productivity\n",
    "\n",
    "This diversity allows the recommendation system to adapt to different user contexts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98bd49f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()\n",
    "df['tags'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98afb3aa",
   "metadata": {},
   "source": [
    "### c. Data Cleaning & Feature Engineering\n",
    "- Categories and context tags were normalized\n",
    "- No missing values were present\n",
    "- Contextual attributes were converted into filters used during recommendation\n",
    "\n",
    "### d. Handling Missing Values or Noise\n",
    "Since the dataset is synthetic and curated, no missing values or noisy entries were found."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1bb0f8b",
   "metadata": {},
   "source": [
    "## 3. Model / System Design\n",
    "\n",
    "### a. AI Technique Used\n",
    "- Recommendation System\n",
    "- Reinforcement Learning (Multi-Armed Bandit)\n",
    "- Hybrid AI (ML + LLM)\n",
    "\n",
    "### b. System Architecture\n",
    "1. User provides context (time, energy, location)\n",
    "2. Activities are filtered from dataset\n",
    "3. Reinforcement learning ranks activities\n",
    "4. Top recommendations are shown\n",
    "5. User feedback updates the learning model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90a7beac",
   "metadata": {},
   "source": [
    "### c. Justification of Design Choices\n",
    "- **Reinforcement Learning** allows the system to learn from user feedback instead of static rules\n",
    "- **Thompson Sampling** balances exploration and exploitation efficiently\n",
    "- **Hybrid LLM usage** prevents cold-start issues and adds creativity\n",
    "- **SQLite storage** enables persistent learning across sessions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7aa74e20",
   "metadata": {},
   "source": [
    "## 4. Core Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84da7746",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import sqlite3\n",
    "from collections import defaultdict\n",
    "\n",
    "# Simplified Bandit Agent with SQL persistence\n",
    "class BanditAgent:\n",
    "    def __init__(self, activities, db_file=\"feedback.db\"):\n",
    "        self.activities = activities\n",
    "        self.db_file = db_file\n",
    "        self.init_db()\n",
    "        self.success = self.load_params('alpha')\n",
    "        self.failure = self.load_params('beta')\n",
    "\n",
    "    def init_db(self):\n",
    "        conn = sqlite3.connect(self.db_file)\n",
    "        c = conn.cursor()\n",
    "        c.execute('''CREATE TABLE IF NOT EXISTS beta_params (\n",
    "                        activity TEXT PRIMARY KEY,\n",
    "                        alpha REAL DEFAULT 1,\n",
    "                        beta REAL DEFAULT 1\n",
    "                    )''')\n",
    "        conn.commit()\n",
    "        conn.close()\n",
    "\n",
    "    def load_params(self, param):\n",
    "        conn = sqlite3.connect(self.db_file)\n",
    "        c = conn.cursor()\n",
    "        c.execute(f\"SELECT activity, {param} FROM beta_params\")\n",
    "        rows = c.fetchall()\n",
    "        conn.close()\n",
    "        params = defaultdict(lambda: 1)\n",
    "        for activity, value in rows:\n",
    "            params[activity] = value\n",
    "        return params\n",
    "\n",
    "    def save_params(self):\n",
    "        conn = sqlite3.connect(self.db_file)\n",
    "        c = conn.cursor()\n",
    "        for act in self.activities:\n",
    "            c.execute(\"INSERT OR REPLACE INTO beta_params (activity, alpha, beta) VALUES (?, ?, ?)\",\n",
    "                      (act, self.success[act], self.failure[act]))\n",
    "        conn.commit()\n",
    "        conn.close()\n",
    "\n",
    "    def recommend(self):\n",
    "        sampled_scores = {\n",
    "            a: np.random.beta(self.success[a], self.failure[a])\n",
    "            for a in self.activities\n",
    "        }\n",
    "        return max(sampled_scores, key=sampled_scores.get)\n",
    "\n",
    "    def update(self, activity, reward):\n",
    "        if reward == 1:\n",
    "            self.success[activity] += 1\n",
    "        else:\n",
    "            self.failure[activity] += 1\n",
    "        self.save_params()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5495edf",
   "metadata": {},
   "source": [
    "The system treats each activity as an \"arm\" in a multi-armed bandit.\n",
    "User feedback acts as the reward signal, allowing the model to improve recommendations over time.\n",
    "The SQLite database persists the beta parameters for learning across sessions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42bd4634",
   "metadata": {},
   "source": [
    "## 5. Evaluation & Analysis\n",
    "\n",
    "### a. Evaluation Metrics\n",
    "Since this is an interactive recommendation system:\n",
    "- Qualitative evaluation was used\n",
    "- User satisfaction based on feedback trends"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fb36964",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = BanditAgent(df['name'].tolist())\n",
    "\n",
    "for _ in range(10):\n",
    "    rec = agent.recommend()\n",
    "    reward = random.choice([0, 1])  # simulated feedback\n",
    "    agent.update(rec, reward)\n",
    "    print(f\"Recommended: {rec}, Reward: {reward}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99d98b22",
   "metadata": {},
   "source": [
    "### b. Performance Analysis\n",
    "- Recommendations become more personalized after multiple interactions\n",
    "- Repeated disliked activities are gradually avoided\n",
    "\n",
    "### c. Limitations\n",
    "- Requires user interaction to learn\n",
    "- Single-user focus"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f001d5f9",
   "metadata": {},
   "source": [
    "## 6. Ethical Considerations & Responsible AI\n",
    "\n",
    "- No personal or sensitive data is collected\n",
    "- Dataset avoids harmful or unsafe activities\n",
    "- Feedback-based learning reduces biased assumptions\n",
    "- LLM usage is controlled and disclosed"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
