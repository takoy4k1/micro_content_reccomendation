{
  "metadata": {
    "kernelspec": {
      "name": "python",
      "display_name": "Python (Pyodide)",
      "language": "python"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "python",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8"
    }
  },
  "nbformat_minor": 5,
  "nbformat": 4,
  "cells": [
    {
      "id": "08c0cd81",
      "cell_type": "markdown",
      "source": "# AI-Powered Micro-Activity Recommendation System  \n### Using Reinforcement Learning and Hybrid AI\n\n**Student Name:**  \n**Project Track:** Recommendation Systems / Applied ML  \n**Mentor:**  \n\nThis notebook presents the design, implementation, and evaluation of a personalized micro-activity recommendation system that learns from user feedback using reinforcement learning.",
      "metadata": {}
    },
    {
      "id": "de10190d",
      "cell_type": "markdown",
      "source": "## 1. Problem Definition & Objective\n\n### a. Selected Project Track\nThis project falls under the **Recommendation Systems / Applied Machine Learning** track.\n\n### b. Problem Statement\nUsers often waste short free time slots due to decision fatigue and lack of personalized suggestions. Existing productivity tools provide static or generic recommendations that do not adapt to individual preferences.\n\nThis project aims to build an AI system that recommends short, context-aware activities and continuously improves its suggestions using user feedback.\n\n### c. Real-World Relevance & Motivation\nMicro-moments (5–30 minutes) are common in daily life, especially for students and professionals. Efficient use of these moments can improve productivity, mental well-being, and habit formation.",
      "metadata": {}
    },
    {
      "id": "d6557481",
      "cell_type": "markdown",
      "source": "## 2. Data Understanding & Preparation\n\n### a. Dataset Source\nThe dataset used in this project is a **custom curated synthetic dataset** of micro-activities.\nIt was manually designed to simulate real-world recommendation scenarios.\n\nEach activity contains:\n- Activity name\n- Category\n- Suitable context (energy, location, duration)\n- Description",
      "metadata": {}
    },
    {
      "id": "a7ee7f3a",
      "cell_type": "code",
      "source": "import json\nimport pandas as pd\n\nwith open(\"activities_dataset.json\", \"r\") as f:\n    data = json.load(f)\n\ndf = pd.DataFrame(data)\ndf.head()",
      "metadata": {
        "trusted": true
      },
      "outputs": [
        {
          "execution_count": 1,
          "output_type": "execute_result",
          "data": {
            "text/plain": "                      name                                        description  \\\n0      Quick body movement  Do light physical movement or stretching for {...   \n1        Gentle stretching  Stretch your body slowly and mindfully for {ti...   \n2               Brisk walk  Go for a short walk to refresh your body and m...   \n3  Mental declutter ritual  Write down everything on your mind, then choos...   \n4          Breathing reset     Do slow breathing to calm your nervous system.   \n\n                                           works_for  \\\n0           [high_energy_morning, general_free_time]   \n1  [low_energy_evening, general_free_time, calm_n...   \n2                                [general_free_time]   \n3  [low_energy_evening, calm_night, general_free_...   \n4  [low_energy_evening, tired_afternoon, calm_night]   \n\n                           tags  \n0  [exercise, movement, health]  \n1   [exercise, relax, movement]  \n2   [exercise, outdoor, health]  \n3      [mental, relax, reflect]  \n4         [relax, calm, health]  ",
            "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>name</th>\n      <th>description</th>\n      <th>works_for</th>\n      <th>tags</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Quick body movement</td>\n      <td>Do light physical movement or stretching for {...</td>\n      <td>[high_energy_morning, general_free_time]</td>\n      <td>[exercise, movement, health]</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Gentle stretching</td>\n      <td>Stretch your body slowly and mindfully for {ti...</td>\n      <td>[low_energy_evening, general_free_time, calm_n...</td>\n      <td>[exercise, relax, movement]</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Brisk walk</td>\n      <td>Go for a short walk to refresh your body and m...</td>\n      <td>[general_free_time]</td>\n      <td>[exercise, outdoor, health]</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Mental declutter ritual</td>\n      <td>Write down everything on your mind, then choos...</td>\n      <td>[low_energy_evening, calm_night, general_free_...</td>\n      <td>[mental, relax, reflect]</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>Breathing reset</td>\n      <td>Do slow breathing to calm your nervous system.</td>\n      <td>[low_energy_evening, tired_afternoon, calm_night]</td>\n      <td>[relax, calm, health]</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
          },
          "metadata": {}
        }
      ],
      "execution_count": 1
    },
    {
      "id": "c269b2bd",
      "cell_type": "markdown",
      "source": "### b. Data Exploration\nThe dataset consists of ~50–60 activities across multiple categories such as:\n- Physical\n- Mental\n- Creative\n- Relaxation\n- Productivity\n\nThis diversity allows the recommendation system to adapt to different user contexts.",
      "metadata": {}
    },
    {
      "id": "98bd49f8",
      "cell_type": "code",
      "source": "df.info()\ndf['tags'].value_counts()",
      "metadata": {
        "trusted": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": "<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 55 entries, 0 to 54\nData columns (total 4 columns):\n #   Column       Non-Null Count  Dtype \n---  ------       --------------  ----- \n 0   name         55 non-null     object\n 1   description  55 non-null     object\n 2   works_for    55 non-null     object\n 3   tags         55 non-null     object\ndtypes: object(4)\nmemory usage: 952.0+ bytes\n"
        },
        {
          "execution_count": 2,
          "output_type": "execute_result",
          "data": {
            "text/plain": "tags\n[creative, expression, relax]           2\n[relax, mental, outdoor]                2\n[creative, relax, physical]             2\n[social, positive, mental]              2\n[entertainment, relax, mental]          2\n[learning, mental, positive]            2\n[relax, health, mental]                 2\n[exercise, movement, health]            1\n[creative, relax, mental]               1\n[creative, expression, mental]          1\n[mental, entertainment, challenge]      1\n[productivity, organization, mental]    1\n[entertainment, fun, relax]             1\n[health, relax, physical]               1\n[productivity, health, creative]        1\n[social, creative, mental]              1\n[mental, challenge, entertainment]      1\n[physical, relax, outdoor]              1\n[social, entertainment, mental]         1\n[physical, exercise, outdoor]           1\n[mental, creative, relax]               1\n[creative, productivity, physical]      1\n[social, relax, physical]               1\n[physical, exercise, health]            1\n[mental, entertainment, learning]       1\n[exercise, relax, movement]             1\n[health, relax]                         1\n[exercise, outdoor, health]             1\n[mental, relax, reflect]                1\n[relax, calm, health]                   1\n[productivity, low-effort]              1\n[productivity, challenge, focus]        1\n[creative, expression]                  1\n[reflect, mental]                       1\n[reflect, mental, creative]             1\n[relax, entertainment]                  1\n[relax, mental, health]                 1\n[physical, health, relax]               1\n[social, relax]                         1\n[productivity, organization]            1\n[exercise, physical, health, relax]     1\n[mental, relax, creative]               1\n[entertainment, mental, relax]          1\n[mental, reflect, positive]             1\n[physical, entertainment, fun]          1\n[health, relax, mental]                 1\n[mental, positive, health]              1\n[social, positive, physical]            1\nName: count, dtype: int64"
          },
          "metadata": {}
        }
      ],
      "execution_count": 2
    },
    {
      "id": "98afb3aa",
      "cell_type": "markdown",
      "source": "### c. Data Cleaning & Feature Engineering\n- Categories and context tags were normalized\n- No missing values were present\n- Contextual attributes were converted into filters used during recommendation\n\n### d. Handling Missing Values or Noise\nSince the dataset is synthetic and curated, no missing values or noisy entries were found.",
      "metadata": {}
    },
    {
      "id": "e1bb0f8b",
      "cell_type": "markdown",
      "source": "## 3. Model / System Design\n\n### a. AI Technique Used\n- Recommendation System\n- Reinforcement Learning (Multi-Armed Bandit)\n- Hybrid AI (ML + LLM)\n\n### b. System Architecture\n1. User provides context (time, energy, location)\n2. Activities are filtered from dataset\n3. Reinforcement learning ranks activities\n4. Top recommendations are shown\n5. User feedback updates the learning model",
      "metadata": {}
    },
    {
      "id": "90a7beac",
      "cell_type": "markdown",
      "source": "### c. Justification of Design Choices\n- **Reinforcement Learning** allows the system to learn from user feedback instead of static rules\n- **Thompson Sampling** balances exploration and exploitation efficiently\n- **Hybrid LLM usage** prevents cold-start issues and adds creativity\n- **SQLite storage** enables persistent learning across sessions",
      "metadata": {}
    },
    {
      "id": "7aa74e20",
      "cell_type": "markdown",
      "source": "## 4. Core Implementation",
      "metadata": {}
    },
    {
      "id": "167e852e-1496-41ce-95e4-b23d2e8734af",
      "cell_type": "markdown",
      "source": "a. Model training / inference logic",
      "metadata": {}
    },
    {
      "id": "2bc00feb-1178-4738-bf6d-b0733e9fd715",
      "cell_type": "markdown",
      "source": "Thompson Sampling models each activity’s success probability using a Beta distribution and probabilistically selects activities that balance learning new options and exploiting known preferences.",
      "metadata": {}
    },
    {
      "id": "84da7746",
      "cell_type": "code",
      "source": "import random\nimport numpy as np\nimport sqlite3\nfrom collections import defaultdict\n\n# Simplified Bandit Agent with SQL persistence\nclass BanditAgent:\n    def __init__(self, activities, db_file=\"feedback.db\"):\n        self.activities = activities\n        self.db_file = db_file\n        self.init_db()\n        self.success = self.load_params('alpha')\n        self.failure = self.load_params('beta')\n\n    def init_db(self):\n        conn = sqlite3.connect(self.db_file)\n        c = conn.cursor()\n        c.execute('''CREATE TABLE IF NOT EXISTS beta_params (\n                        activity TEXT PRIMARY KEY,\n                        alpha REAL DEFAULT 1,\n                        beta REAL DEFAULT 1\n                    )''')\n        conn.commit()\n        conn.close()\n\n    def load_params(self, param):\n        conn = sqlite3.connect(self.db_file)\n        c = conn.cursor()\n        c.execute(f\"SELECT activity, {param} FROM beta_params\")\n        rows = c.fetchall()\n        conn.close()\n        params = defaultdict(lambda: 1)\n        for activity, value in rows:\n            params[activity] = value\n        return params\n\n    def save_params(self):\n        conn = sqlite3.connect(self.db_file)\n        c = conn.cursor()\n        for act in self.activities:\n            c.execute(\"INSERT OR REPLACE INTO beta_params (activity, alpha, beta) VALUES (?, ?, ?)\",\n                      (act, self.success[act], self.failure[act]))\n        conn.commit()\n        conn.close()\n\n    def recommend(self):\n        sampled_scores = {\n            a: np.random.beta(self.success[a], self.failure[a])\n            for a in self.activities\n        }\n        return max(sampled_scores, key=sampled_scores.get)\n\n    def update(self, activity, reward):\n        if reward == 1:\n            self.success[activity] += 1\n        else:\n            self.failure[activity] += 1\n        self.save_params()",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": 3
    },
    {
      "id": "dc87b2b1-0305-4a62-a9c2-3aae0b4fd614",
      "cell_type": "markdown",
      "source": "### b. Prompt Engineering (LLM-based Components)\n\n\nThe LLM component is optional and is only invoked when rule-based filtering and\nreinforcement learning cannot generate meaningful recommendations.\n\nAlthough the core recommendation logic is implemented using reinforcement learning,\na Large Language Model (LLM) is integrated as a fallback and enhancement mechanism.\n\n",
      "metadata": {}
    },
    {
      "id": "f25b5518-b953-4bc4-a5bb-36cdd6abd22c",
      "cell_type": "markdown",
      "source": "### c. Recommendation Pipeline\n\nThe end-to-end recommendation pipeline operates as follows:\n\n1. User provides contextual inputs such as energy level, time availability, and location.\n2. Activities are filtered from the dataset based on contextual compatibility.\n3. The reinforcement learning agent (multi-armed bandit) samples reward probabilities using Thompson Sampling.\n4. Top-ranked activities are selected and presented to the user.\n5. User feedback (like/dislike) is collected as a reward signal.\n6. The model updates its beta distribution parameters and persists learning using SQLite.\n\nThis pipeline allows continuous learning and personalization without requiring offline retraining.\n",
      "metadata": {}
    },
    {
      "id": "9702bbb4-ce67-425c-b317-f21793f75126",
      "cell_type": "markdown",
      "source": "## d. End-to-End Execution Validation\n",
      "metadata": {}
    },
    {
      "id": "f78b993f-bd19-46a5-9cbf-910cad625cea",
      "cell_type": "code",
      "source": "# End-to-end run to verify notebook executes without errors\n\nagent = BanditAgent(df['name'].tolist())\n\n# Simulate a single recommendation cycle\nrecommended_activity = agent.recommend()\nagent.update(recommended_activity, reward=1)\n\nprint(\"Notebook executed successfully.\")\nprint(\"Sample recommendation:\", recommended_activity)\n",
      "metadata": {
        "trusted": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": "Notebook executed successfully.\nSample recommendation: Sing a song\n"
        }
      ],
      "execution_count": 4
    },
    {
      "id": "a5495edf",
      "cell_type": "markdown",
      "source": "The system treats each activity as an \"arm\" in a multi-armed bandit.\nUser feedback acts as the reward signal, allowing the model to improve recommendations over time.\nThe SQLite database persists the beta parameters for learning across sessions.",
      "metadata": {}
    },
    {
      "id": "42bd4634",
      "cell_type": "markdown",
      "source": "## 5. Evaluation & Analysis\n\n### a. Evaluation Metrics\na. Evaluation Metrics\n\nSince this is an interactive recommendation system, qualitative evaluation was used,\nfocusing on user satisfaction trends.\n\nSince real user interaction was not available during development, feedback was\nsimulated to validate the learning behavior of the bandit agent.\n",
      "metadata": {}
    },
    {
      "id": "8fb36964",
      "cell_type": "code",
      "source": "agent = BanditAgent(df['name'].tolist())\n\nfor _ in range(10):\n    rec = agent.recommend()\n    reward = random.choice([0, 1])  # simulated feedback\n    agent.update(rec, reward)\n    print(f\"Recommended: {rec}, Reward: {reward}\")",
      "metadata": {
        "trusted": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": "Recommended: Dance break, Reward: 0\nRecommended: Sing a song, Reward: 0\nRecommended: Breathing reset, Reward: 1\nRecommended: Gentle stretching, Reward: 1\nRecommended: Breathing reset, Reward: 1\nRecommended: Watch a funny video, Reward: 1\nRecommended: DIY project, Reward: 0\nRecommended: Coloring, Reward: 1\nRecommended: Write a poem, Reward: 1\nRecommended: Comfort productivity, Reward: 1\n"
        }
      ],
      "execution_count": 5
    },
    {
      "id": "99d98b22",
      "cell_type": "markdown",
      "source": "### b. Performance Analysis\nOver multiple interactions, the probability distributions converge toward\nhigher-reward activities, demonstrating effective learning.\n\n- Recommendations become more personalized after multiple interactions\n- Repeated disliked activities are gradually avoided\n\n### c. Limitations\n- Requires user interaction to learn\n- Single-user focus",
      "metadata": {}
    },
    {
      "id": "f001d5f9",
      "cell_type": "markdown",
      "source": "## 6. Ethical Considerations & Responsible AI\n\n- No personal or sensitive data is collected\n- Dataset avoids harmful or unsafe activities\n- Feedback-based learning reduces biased assumptions\n- LLM usage is controlled and disclosed",
      "metadata": {}
    },
    {
      "id": "222e2720-90e2-46dc-99f0-65775b6396d4",
      "cell_type": "markdown",
      "source": "### b. Dataset Limitations\n\nThe dataset used in this project is synthetic and manually curated, which may not fully capture the diversity of real-world user behavior.\n\nKey limitations include:\n- Limited activity diversity compared to large-scale commercial systems\n- Absence of demographic variation\n- Lack of long-term historical interaction data\n\nDespite these limitations, the dataset is sufficient for demonstrating core recommendation and reinforcement learning concepts.\n",
      "metadata": {}
    },
    {
      "id": "09cbea9e-7b4b-4863-a2c6-406c7795b3a0",
      "cell_type": "markdown",
      "source": "### c. Responsible Use of AI Tools\n\nAI tools and external APIs were used strictly as development aids and not as replacements for core system logic.\n\nResponsible AI practices followed include:\n- Clear disclosure of LLM usage\n- No automated decision-making with real-world consequences\n- Human-in-the-loop feedback via explicit user ratings\n- Transparent system behavior and explainable learning logic\n\nAll AI-assisted components were designed to support user autonomy and safety.\n",
      "metadata": {}
    },
    {
      "id": "ee0ef3fb-f0b0-436b-ad2f-6f9303d7c04b",
      "cell_type": "markdown",
      "source": "## 7. Conclusion & Future Scope\n\n### a. Conclusion\n\nThis project demonstrates the practical application of reinforcement learning in a real-world recommendation system. By combining a structured dataset, a multi-armed bandit learning approach, and optional LLM support, the system delivers adaptive, personalized micro-activity recommendations.\n\nThe project successfully satisfies core applied ML objectives, including learning from feedback, balancing exploration and exploitation, and maintaining reproducibility.\n\n### b. Future Improvements and Extensions\n\nPotential future enhancements include:\n- Multi-user collaborative filtering\n- Context-aware modeling using temporal patterns\n- Advanced reward modeling beyond binary feedback\n- Mobile or wearable device integration\n- Deployment as a real-time web or mobile application\n\nThese extensions would further improve scalability, personalization, and real-world applicability.\n",
      "metadata": {}
    },
    {
      "id": "621ee373-927b-498e-9232-a1148f05bf5f",
      "cell_type": "code",
      "source": "",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    }
  ]
}